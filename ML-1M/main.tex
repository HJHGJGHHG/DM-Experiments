\input{Style/style}
\begin{document}
\title{数据挖掘第一次上机实验报告}
\author{刘臻劼}

\input{Content/titlepage.tex} % 这个是封面，不需要刻意注释掉

\clearpage
\tableofcontents
\pagestyle{preContent}

\clearpage
\section{实验内容}
\subsection{实验目标}
\xiaosi\begin{spacing}{1.5} 本次实验目标为：在MovieLens数据集上构建“用户——电影”评分矩阵，基于评分矩阵对用户与电影进行推荐。
\end{spacing}
\subsection{数据集介绍}
\xiaosi\begin{spacing}{1.5}
MovieLens数据集包含6040个用户关于3883部电影的1000209条评分信息。数据集分为三个文件：用户数据users.dat、电影数据movies.dat和评分数据ratings.dat。\par
（1）用户数据\par
关于6040个用户的个人脱敏数据，格式为
{\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}
\begin{align}
UserID::Gender::Age::Occupation::Zip-code
\nonumber\end{align}
}
其中：\par
$UserID$：1到6040顺序编号；\par
$Gender$：F为女性，M为男性；\par
$Age$：分为1,18,25,35,45,50,56共7类，代表用户所属的年龄段；\par
$Occupation$：0到20的整数，分别代表21种职业；\par
$Zip-code$：邮编。\par
（2）电影数据\par
关于3883部电影的信息，格式为
{\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}
\begin{align}
MovieID::Title::Genres
\nonumber\end{align}
}
其中：\par
$MovieID$：1到3952无重复的整数，非连续编号；\par
$Title$：电影名，为电影名与年份的拼接，如“Toy Story (1995)”；\par
$Genres$：电影所属的类别，如果同属多个类别，则类名间用“|”隔开，如：“Adventure|Children's”。\par
（3）评分数据\par
用户对电影的1000209条评分数据，格式为
{\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}
\begin{align}
UserID::MovieID::Rating::Timestamp
\nonumber\end{align}
}
其中：\par
$UserID,\ MovieID$：同上；\par
$Rating$：1到5之间（含）的整数；\par
$Timestamp$：自1970年1月1日零点后到用户提交评价的时间的秒数。
\end{spacing}


\clearpage
\section{实验分析}
\subsection{题目分析}
\xiaosi\begin{spacing}{1.5}
原问题包含两个要求，一是对题给的用户——电影打分数据集进行建模。再是完善评分矩阵并进行推荐。\par
\begin{wrapfigure}{r}{5cm}%靠文字内容的右侧
\centering
\includegraphics[width=0.27\textwidth]{Figure/simrank-f1.png}
\caption{二部图模型}
\end{wrapfigure}
（1）用户——电影打分建模\par
我们考虑用户集 $U=\{u_1,u_2,\cdots u_n\},n=6040$，电影集 $V=\{v_1,v_2,\cdots,v_m\},m=3883$。则用户对电影的评分可视作以 $U,\ V$ 构建的二部图 $G=<U,V,E>$ 相应边的权重，其中边集 $E \subseteq U \times V$，则第$i$个用户对第$j$部电影的评分 $r_{ij}=h(u_i,v_j),\ h:E\rightarrow [0,5]$，此处评分函数的值域为$[0,5]$，将原始任务中的多类分类问题转化为了回归问题，这是考虑到评分允许浮点更加自然。\par
（2）评分矩阵\par
在（1）我们知道边集 $E \subseteq U \times V$，而实际上 $|E|\ll |U|\cdot|V|$，这说明全体用户对所有电影的评价矩阵 $\mathbf{R}=(r_{ij})_{m\times n}$ 是较稀疏的。而本实验的核心问题则是如何通过现有数据补全评价矩阵$\mathbf{R}$。补全评分矩阵后我们将基于此进行推荐。
\end{spacing}

\subsection{推荐算法简介}
\subsubsection{推荐算法的分类}
\xiaosi\begin{spacing}{1.5}
所谓推荐算法，即是根据用户行为推测其喜好的一类算法。推荐算法历史悠久，在机器学习还没有兴起的时候就有需求和应用了。概括来说，可以分为以下5种\cite{1}：\par
（1）基于内容的推荐：这一类一般依赖于NLP方法，通过挖掘文本的TF-IDF特征向量，来得到用户的偏好，进而做推荐。\par
（2）协调过滤推荐：协调过滤是推荐算法中目前最主流的种类，花样繁多，目前绝大多数实际应用的推荐算法都是协同过滤推荐算法。\par
（3）混合推荐：类似机器学习中的集成学习，博才众长，通过多个推荐算法的结合，得到一个更好的推荐算法。\par
（4）基于规则的推荐：这类算法常见的比如基于最多用户点击，最多用户浏览等，属于大众型的推荐方法，在目前的大数据时代并不主流。\par
（5）基于知识的推荐：在某种程度是可以看成是一种推理技术，它不是建立在用户需要和偏好基础上推荐的。\par
\end{spacing}
\subsubsection{协同过滤推荐算法}
\xiaosi\begin{spacing}{1.5}
我们接下来重点讨论协同过滤的推荐算法。协同过滤分析用户兴趣，在用户群中找到指定用户的相似（兴趣）用户，综合这些相似用户对某一信息的评价，形成对该指定用户对此信息的喜好程度预测。\par
协同过滤问题一般这样呈现：只有部分用户和部分对象之间是有评分数据的，其它部分评分是空白，此时我们要用已有的部分稀疏数据来预测那些空白的物品和数据之间的评分关系，找到最高评分的物品推荐给用户。很明显本实验属于一协同过滤问题。
\end{spacing}

\subsection{模型设计}
\xiaosi\begin{spacing}{1.5}
明确题意后，我们来讨论解决方法。针对本题考虑使用基于图论的方法与基于矩阵分解及深度学习方法。
\subsubsection{图论方法}
由 2.1 节，我们将用户——电影建模为二部图，我们可以考虑使用基于图的 SimRank \cite{2}算法。其核心思想是\cite{3}：如果两个用户相似，则与这两个用户相关联的物品也类似；如果两个物品类似，则与这两个物品相关联的用户也类似。\par
（1）相似度矩阵\par
我们期望构建两个相似度矩阵 $\mathbf{X}_{m\times m},\mathbf{Y}_{n\times n}$。其中 $x_{ij} = s(u_i,u_j)$，$y_{i'j'}=s(v_{i'},v_{j'})$。\par
考虑我们在 2.1 节中的建模： $G=<U,V,E>$，则用户 $u_i,\ u_j,\ (i,j=1,2,\cdots,n)$ 的相似度为：
{\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}
\begin{align}
s(u_i,u_j)=\frac{C}{|V(u_i)||V(u_j)|}\cdot \sum_{p=1}^{|V(u_i)|}\sum_{q=1}^{|V(u_j)|} {s(V_p(u_i),V_q(u_j))}
\end{align}
}
其中：$C$ 为阻尼系数；$V(u_i)$ 为用户 $u_i$ 点评过的电影集合，即 $V$ 中与结点 $u_i$ 相连的点的集合，$V(u_j)$ 同理；$s(V_p(u_i),V_q(u_j))$ 为 $V(u_i)$ 中第 $p$ 部电影与 $V(u_j)$ 中第 $q$ 部电影的相似度。\par
对于电影 $v_{i'},\ v_{j'}\ (i',j'=1,2,\cdots,m)$ 的相似度我们同样有：
{\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}
\begin{align}
s(v_{i'},v_{j'})=\frac{C}{|U(v_{i'})||U(v_{j'})|}\cdot \sum_{p=1}^{|U(v_{i'})|}\sum_{q=1}^{|U(v_{j'})|} {s(U_p(v_{i'}),U_q(v_{j'}))}
\end{align}
}\par
从中我们可以看到两个体 $a,b$ 间的相似度取决于与他们相连的所有结点间的相似度。\par
考虑特殊情况：我们定义自身与自身的相似度为1，且若一用户没有评论任何电影，或一部电影没有任何用户评论，则该用户/电影与其他用户/电影的相似度为0。\par
（2）选择矩阵与评分矩阵\par
在求解相似度矩阵前，有必要对几个概念进行阐述。\par
选择矩阵 $\mathbf{C}=(c_{ij})_{m\times n}$：刻画用户 $u_i$ 是否对电影 $v_j$ 有评分。即：
{\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}
\begin{align}
c_{ij} = 
\begin{cases}
1 &<u_i,v_j>\ \in E \\
0 &otherwise
\end{cases}
\end{align}
}\par
评分矩阵 $\mathbf{R}=(r_{ij})_{m\times n}$：刻画用户 $u_i$ 是否对电影 $v_j$ 的评分值。即：
{\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}
\begin{align}
r_{ij} = 
\begin{cases}
rating(u_i,v_j) &<u_i,v_j>\ \in E \\
0 &otherwise
\end{cases}
\end{align}
}\par
（3）相似度矩阵的求解\par
我们以用户间相似度为例，求解 $\mathbf{X}_{m\times m}$。考虑迭代求解，则式（1）可写成：
{\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}
\begin{align}
s_{k+1}(u_i,u_j)=\frac{C}{|V(u_i)||V(u_j)|}\cdot \sum_{p=1}^{|V(u_i)|}\sum_{q=1}^{|V(u_j)|} {s_k(V_p(u_i),V_q(u_j))}
\end{align}
}
其中 $k$ 为迭代轮次数。我们有：
{\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}
\begin{align}
s_{k+1}(u_i,u_j)&=\frac{C}{|V(u_i)||V(u_j)|}\cdot \sum_{p=1}^{|V|}\sum_{q=1}^{|V|} {c_{ip}\cdot c_{jq}\cdot s(v_p,v_q)}\\
&=C\sum_{p=1}^{|V|}\sum_{q=1}^{|V|}(\frac{c_{ip}}{\sum_{e_1=1}^{n}c_{ie_1}})s(v_p,v_q)(\frac{c_{jq}}{\sum_{e_2=1}^n c_{je_2}})
\end{align}
}\par
注意到 $s(v_p,v_q)=y_{pq}$，所以（7）式可写成矩阵的形式：
{\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}
\begin{align}
\mathbf{X}_{k+1} = C\cdot\mathbf{C}^{row}\cdot \mathbf{Y_k} \cdot \mathbf{C}^{row\ T}+ \mathbf{E}_{m\times m} - \Lambda ({C\cdot\mathbf{C}^{row}\cdot \mathbf{Y_k} \cdot \mathbf{C}^{row\ T}})
\end{align}
}
其中：$C$ 为阻尼系数；$\mathbf{C}^{row}$ 为选择矩阵按行归一化；$\mathbf{E}_{m\times m}$ 为 $m\times m$ 的单位矩阵；$\Lambda$ 为只保留矩阵的对角元素而其他元素置0。公式后半部分的 $\mathbf{E}_{m\times m} - \Lambda ({C\cdot\mathbf{C}^{row}\cdot \mathbf{Y_k} \cdot \mathbf{C}^{row\ T}})$ 为将矩阵主对角线元素置1，这是考虑到任意用户与自身的相似度为1。\par
同理，电影相似度矩阵也有迭代公式：
{\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}
\begin{align}
\mathbf{Y}_{k+1} = C\cdot\mathbf{C}^{row\ T}\cdot \mathbf{X_k} \cdot \mathbf{C}^{row}+ \mathbf{E}_{n\times n} - \Lambda (C\cdot\mathbf{C}^{row\ T}\cdot \mathbf{X_k} \cdot \mathbf{C}^{row})
\end{align}
}\par
（4）评分矩阵的补全\par
根据（8）、（9）式经过多次迭代基本稳定后我们即得到了$\mathbf{X},\mathbf{Y}$，接下来我们基于此补全评分矩阵 $\mathbf{R}_{m\times n}$。为了充分利用已知信息，对于 $r_{ij},\ <u_i,v_j>\notin E$，同时使用用户 $u_i$ 与电影 $v_j$ 的所有评分。于是有：
{\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}
\begin{align}
r_{ij} = 
\frac{\sum_{p=1}^n c_{ip}\cdot r_{ip}\cdot s(v_p,v_j) + \sum_{q=1}^{m} c_{qj}\cdot r_{qj}\cdot s(u_q,u_i)}{\sum_{e_1 = 1}^n c_{ie_1}+ \sum_{e_2 = 1}^m c_{e_2j}}
\end{align}
}\par
\subsubsection{矩阵分解结合深度学习}
协同过滤中另一经典算法为矩阵分解。我们希望评分矩阵有如下分解： 
{\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}
\begin{align}
\mathbf{R}_{m\times n}=\mathbf{P}_{m\times k}\mathbf{Q}_{k\times n}
\end{align}
}
而在此处我们不使用传统矩阵分解方法来求解矩阵 $\mathbf{P}$ 与 $\mathbf{Q}$，而是通过深度学习方法得到。\par
传统方法以及上文提到的图论方法仅利用了用户对电影的评分数据，其他诸如用户性别，电影名及类别等等信息则没有利用。通过深度学习方法可以更充分地利用全部信息。对于式（1），我们可以认为 $\mathbf{P}$ 为用户特征矩阵，$\mathbf{Q}$ 为电影特征矩阵。\par
（1）模型概览
模型的概览图如下：
\clearpage
\begin{figure}
\setlength{\abovecaptionskip}{0.cm}
\setlength{\belowcaptionskip}{-0.cm}
\centering
\includegraphics[width=0.8\textwidth]{Figure/2.png}
\caption{模型示意图}
\end{figure}

\par

（2）用户特征提取\par
用户信息中，对于 $Gender,\ Age,\ Job$ 三个字段的信息，由于类别数较少可以采用 One-Hot 编码。而 $UserID$ 由于类别太多故而此处采用Embedding方法，用$embedding\_ dim=32$ 维向量表示用户的ID值。\par
将 $UserID$ 的Embedding向量与其他信息One-Hot向量拼接并通过全连接层并通过 ReLU激活，而后再次通过全连接层并进行批归一化。至此得到了用户信息的特征矩阵。\par
（3）电影特征提取\par
对于电影信息中 $MovieID$ 与 $MovieType$ 字段，类似用户信息的处理。ID通过Embedding得到，电影类型为Multi-Hot向量。\par
而对于电影名称，此处采用CNN提取特征。CNN在NLP的应用最早来源于Kim在2014年的工作\cite{4}。相比CV领域，TextCNN结构较为简单，一般仅有词嵌入、卷积、池化与全连接四部分。而本文用其提取电影文本特征，仅包含词嵌入、卷积与池化三个部分。最后再将电影名称信息与电影信息拼接通过全连接层，得到电影信息的特征矩阵。\par
（4）拟合评分
上述步骤完成后，我们得到了用户特征矩阵 $\mathbf{P} \in \mathbb{R}^{m \times hidden\_ size}$ 与电影特征矩阵 $\mathbf{Q} \in \mathbb{R}^{n \times hidden\_ size}$。则评分矩阵为：
{\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}
\begin{align}
\mathbf{R}_{m\times n}=\mathbf{P}_{m\times h}\mathbf{Q}_{n\times h}^T
\end{align}
}
其中 $h$ 为隐藏层维度。之后通过计算与目标值的MSE误差得到梯度并反向传播进行优化。
\end{spacing}

\section{具体实现}
\subsection{初步数据预处理}
\xiaosi\begin{spacing}{1.5}
首先读取并加载数据：
\begin{lstlisting}
# 加载电影数据
movie_names = ['movie_id', 'movie_title', 'movie_type'] 
movie = pd.read_table('movies.dat', sep='::', header=None,
                      names=movie_names, engine='python') 
# 加载用户数据
user_names = ['user_id', 'user_gender', 'user_age', 'user_job', 'zip']
user = pd.read_table('users.dat', sep='::', header=None,
                     names=user_names, engine='python') 
# 加载评分数据                     
rate_names = ['user_id', 'movie_id', 'rank', 'timestamp']
rating = pd.read_table('ratings.dat', sep='::',
                       header=None, names=rate_names, engine='python')

\end{lstlisting}\par
而后剔除无关列：邮编 $Zip-code$ 与 时间戳 $Timestamp$ 是我们不需要的。
\begin{lstlisting}
user = user.drop(['zip'], axis=1)
rating = rating.drop(['timestamp'], axis=1)
\end{lstlisting}\par
再然后处理用户数据，将 $Gender$，$Age$，$Job$ 转换为 One-Hot向量。
\begin{lstlisting}
# User 相关数据处理
user['user_gender'] = user['user_gender'].apply(lambda x: [1, 0] if x == 'F' else [0, 1])

def convert_age_to_One_Hot(age):
    if age == 1:
        return [1, 0, 0, 0, 0, 0, 0]
    elif age == 18:
        return [0, 1, 0, 0, 0, 0, 0]
    elif age == 25:
        return [0, 0, 1, 0, 0, 0, 0]
    elif age == 35:
        return [0, 0, 0, 1, 0, 0, 0]
    elif age == 45:
        return [0, 0, 0, 0, 1, 0, 0]
    elif age == 50:
        return [0, 0, 0, 0, 0, 1, 0]
    else:
        return [0, 0, 0, 0, 0, 0, 1]

def convert_job_to_One_Hot(job):
    jobs = [0] * 21
    jobs[job] += 1
    return jobs

user['user_age'] = user['user_age'].apply(convert_age_to_One_Hot)
user['user_job'] = user['user_job'].apply(convert_job_to_One_Hot)
\end{lstlisting}\par
接下来处理电影数据。将电影类别转换为Multi-Hot向量，而后构建电影标题文本词典，将其索引化。
\begin{lstlisting}
# Movie 相关数据处理
# 电影名称索引化
args.max_length = 16
movie_title_word2id = {'pad': 0}
for i in range(len(movie['movie_title'])):
    words = movie['movie_title'][i].split(' ')
    del words[-1]  # 去除年份
    movie_title_id = []
    for word in words:
        if word not in movie_title_word2id:
            movie_title_word2id[word] = len(movie_title_word2id)
        movie_title_id.append(movie_title_word2id[word])
    movie_title_id.extend([0] * (args.max_length - len(words)))  # 填充
    movie['movie_title'].loc[i] = movie_title_id
args.vocabulary_size = len(movie_title_word2id)

# 电影类型
for i in range(len(movie['movie_type'])):
    types = movie['movie_type'][i].split('|')
    type_id = []
    for j in range(len(movie_types)):
        if movie_types[j] in types:
            type_id.append(1)
        else:
            type_id.append(0)
    movie['movie_type'].loc[i] = type_id
\end{lstlisting}\par
之后融合三个DataFrame中的数据作为初步预处理的结果。
\begin{lstlisting}
tmp = pd.merge(rating, user)
data = pd.merge(tmp, movie)
\end{lstlisting}
\par
\subsection{数据集探索与划分}
我们注意到一共有3883部电影但有用户评论的电影数仅3706部，亦即有177部电影无用户评论。在本题的二分图建模中，这些电影对应结点为孤立节点，此处将其删去。故而 $n$ 值改变为 3706。
\begin{lstlisting}
movies = {}
users = {}
for i in range(len(data)):
    sample = data.iloc[i]
    if sample['user_id'] not in users.keys():
        users[sample['user_id']] = {'uid': sample['user_id']}
    if sample['movie_id'] not in movies.keys():
        movies[sample['movie_id']] = {'mid': sample['movie_id']}
mid_rated = movies.keys() # 3706
total_mid = movie['movie_id'].values # 3883
mid_not_rated = [x for x in total_mid if x not in mid_rated] # 177
\end{lstlisting}\par
再从中抽取10000条数据用于模型测试并根据训练集构建选择矩阵 $\mathbf{C}_{6040\times 3706}$ 与初始评分矩阵 $\mathbf{R}_{6040\times 3706}$。
\begin{lstlisting}
choice_matrix = pd.DataFrame(np.zeros([6040, 3706], dtype=float))
rank_matrix_initial = pd.DataFrame(np.zeros([6040, 3706], dtype=float))

for i in range(len(train)):
    sample = train.iloc[i]
    uid = sample['user_id']
    mid = sample['movie_id']
    rank = sample['rank']
    user_index = user_index_to_uid.index(uid)
    movie_index = movie_index_to_mid.index(mid)
    choice_matrix[movie_index][user_index] = 1
    rank_matrix_initial[movie_index][user_index] = rank

pkl.dump(choice_matrix, open('choice_matrix.pkl', 'wb'))
pkl.dump(rank_matrix_initial, open('rank_matrix_initial.pkl', 'wb'))
\end{lstlisting}\par

\subsection{矩阵分解结合深度学习方法}
\par
构建学习用户特征矩阵与电影特征矩阵的深度学习模型。
\begin{lstlisting}
class MovieLens(nn.Module):
    def __init__(self, user_max_dict, movie_max_dict, args):
        super(MovieLens, self).__init__()
        self.args = args
        # ------------ user channel ------------
        # user embeddings
        self.embedding_uid = nn.Embedding(user_max_dict['uid'], args.embedding_dim, device=args.device)
        
        # user info NN
        self.user_layer = nn.Sequential(
            nn.Dropout(args.dropout),
            nn.Linear(
                args.embedding_dim + user_max_dict['gender'] + user_max_dict['age'] + user_max_dict['job'],
                args.hidden_dim, device=args.device),
            nn.ReLU(),
            
            nn.Linear(args.hidden_dim, args.hidden_dim, device=args.device),
            nn.Tanh(),
            nn.BatchNorm1d(args.hidden_dim, device=args.device)
        )
        
        # ------------ movie channel ------------
        # movie embeddings
        self.embedding_mid = nn.Embedding(movie_max_dict['mid'], args.embedding_dim, device=args.device)  # normally 32
        
        # movie info NN
        self.movie_layer = nn.Sequential(
            nn.Linear(args.embedding_dim + movie_max_dict['mtype'], args.hidden_dim, device=args.device),
            nn.ReLU(),
            nn.BatchNorm1d(args.hidden_dim, device=args.device)
        )
        
        # movie title text
        self.text_embedding = get_movie_text_embedding(args)
        self.convs = nn.ModuleList(
            [nn.Conv2d(1, args.hidden_dim, (size, args.embedding_dim), device=args.device) for size in
             args.filter_sizes])
        self.dropout = nn.Dropout(args.dropout)
        
        self.movie_combine_layer = nn.Sequential(
            nn.Linear(args.hidden_dim + len(args.filter_sizes) * args.hidden_dim, args.hidden_dim, device=args.device),
            nn.Tanh()
        )
    
    def forward(self, user_input, movie_input):
        # ------------ user channel ------------
        uid = torch.squeeze(
            self.embedding_uid(user_input['uid'].to(self.args.device)), dim=1)
        uid = torch.squeeze(uid, dim=1)  # [batch_size, embedding_dim]
        gender = torch.squeeze(user_input['gender'].to(self.args.device), dim=1)  # [batch_size, gender_types (2)]
        age = torch.squeeze(user_input['age'].to(self.args.device), dim=1)  # [batch_size, age_types (7)]
        job = torch.squeeze(user_input['job'].to(self.args.device), dim=1)  # [batch_size, job_types (21)]
        user_info = torch.cat([uid, gender, age, job],
                              dim=1)  # concat of user info tensors  [batch_size, sum of dimensions]
        user_feature = self.user_layer(user_info)  # user feature  [batch_size, hidden_dim]
        
        # ------------ movie channel ------------
        mid = torch.squeeze(
            self.embedding_mid(movie_input['mid'].to(self.args.device)), dim=1)
        mid = torch.squeeze(mid, dim=1)  # [batch_size, embedding_dim]
        mtype = movie_input['mtype'].to(self.args.device)  # [batch_size, movie_types (18)]
        movie_info = torch.cat([mid, mtype], 1)  # concat of movie_id tensor and movie_type tensor
        movie_info = self.movie_layer(movie_info)  # movie info  [batch_size, hidden_dim]
        
        # movie title text
        mtext = movie_input['mtext'].to(self.args.device)  # [batch_size, seq_len (16)]
        text_embedding = self.text_embedding(mtext).unsqueeze(1)  # [batch_size, seq_len, text_embedding_dim]
        text_info = [F.relu(conv(text_embedding)).squeeze(3) for conv in self.convs]
        text_info = [F.max_pool1d(item, item.size(2)).squeeze(2) for item in text_info]
        text_info = self.dropout(
            torch.cat(text_info, 1))  # movie text info  [batch_size, hidden_dim * len(filter_sizes)]
        
        movie_info = torch.cat([movie_info, text_info], dim=1)
        movie_feature = self.movie_combine_layer(movie_info)
        output = torch.sum(user_feature * movie_feature, 1)  # [batch_size, 1]
        
        return output, user_feature, movie_feature
\end{lstlisting}\par
深度学习模型的训练：
\begin{lstlisting}
def load_model(args):
    set_seed(args)
    train_datasets = MovieRankDataset(pkl_file=args.path + 'train.pkl', args=args)
    test_datasets = MovieRankDataset(pkl_file=args.path + 'test.pkl', args=args)
    train_iter = DataLoader(train_datasets, batch_size=args.batch_size, drop_last=True)
    test_iter = DataLoader(test_datasets, batch_size=args.batch_size)
    model = MovieLens(user_max_dict=user_max_dict, movie_max_dict=movie_max_dict, args=args).to(args.device)
    return train_iter, test_iter, model


def train(args, model, train_iter):
    # optimizer
    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
         'weight_decay': 0.01},
        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
    ]
    optimizer = Adam(optimizer_grouped_parameters, lr=args.lr)
    # criterion
    criterion = nn.MSELoss()
    writer = SummaryWriter(logdir='/root/tf-logs')
    for epoch in range(args.epochs):
        print('Epoch [{}/{}]'.format(epoch + 1, args.epochs))
        model = model.train()
        losses = []
        for i_batch, sample_batch in enumerate(train_iter):
            user_inputs = sample_batch['user_inputs']
            movie_inputs = sample_batch['movie_inputs']
            target = torch.squeeze(sample_batch['target'].to(args.device))
            
            rank, _, _ = model(user_inputs, movie_inputs)
            loss = criterion(rank, target)
            losses.append(loss.item())
            loss.backward()
            if i_batch % 100 == 0:
                writer.add_scalar('data/loss', loss, i_batch * 20)
                print('Steps: {}\{}  Loss: {}'.format(i_batch, len(train_iter), loss.item()))
            
            # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            optimizer.zero_grad()
        # 一个Epoch训练完毕，输出train_loss
        print('Epoch: {0}   Train Loss: {1:>5.6}'.format(epoch + 1, np.mean(losses)))
    
    writer.export_scalars_to_json("./test.json")
    writer.close()
\end{lstlisting}\par
\subsection{SimRank算法}
\begin{lstlisting}
def simrank(args):
    user_index_to_uid = pkl.load(open(args.path + 'user_index_to_uid.pkl', 'rb'))
    movie_index_to_mid = pkl.load(open(args.path + 'movie_index_to_mid.pkl', 'rb'))
    R = pkl.load(open(args.path + 'rank_matrix_initial.pkl', 'rb'))
    C = pkl.load(open(args.path + 'choice_matrix.pkl', 'rb'))  # Choice Matrix
    C_col = C.apply(normalization, axis=0)  # 列归一化
    C_row = C.apply(normalization, axis=1)  # 行归一化
    X = pd.DataFrame(np.zeros([6040, 6040], dtype=int))
    Y = pd.DataFrame(np.zeros([3706, 3706], dtype=int))
    
    for k in range(args.k):
        X_new = args.c * np.dot(np.dot(C_row, Y), np.matrix(C_row).T) + np.eye(6040) - np.diag(
            np.diagonal(args.c * np.dot(np.dot(C_row, Y), np.matrix(C_row).T)))
        Y_new = args.c * np.dot(np.dot(C_col.T, X), C_col) + np.eye(3706) - np.diag(
            np.diagonal(args.c * np.dot(np.dot(C_col.T, X), C_col)))
        X = X_new
        Y = Y_new
    
    def series_notzero_values(series):
        val = 0
        for i in series.index:
            if series[i] > 0:
                val += 1
        return val
    
    for i in R.index:
        for j in R.columns:
            if R.iloc[i][j] > 0:
                pass
            else:
                val = 0
                for p in R.columns:
                    val += C.iloc[i][p] * R.iloc[i][p] * Y[p, j]
                for q in R.index:
                    val += C.iloc[q][j] * R.iloc[q][j] * X[q, i]
                not_zero_val = series_notzero_values(C[j]) + series_notzero_values(C.iloc[i])
                # not_zero_val = series_notzero_values(C.iloc[i])
                R[j][i] = val / not_zero_val
    
    return R
\end{lstlisting}\par
\subsection{推荐}
\begin{lstlisting}
def recommend_Movies(args, uid, rank_matrix):
    """
    根据 UserID 为其推荐电影与也喜欢看这些电影的用户 (相似用户)
    """
    users_raw, movies_raw, _, _ = load_preprocessed_data(args)
    user_ratings = rank_matrix.loc[uid].sort_values(ascending=False)
    top_k_movies = list(user_ratings.index)[0:args.recommend_num]
    similar_users = []
    print('该用户信息：' + 'UserID: {0}    Gender:{1}    Age:{2}    Occupation:{3}'.format(
        uid, user_gender[users_raw[uid]['Gender']], user_age[users_raw[uid]['Age']], user_job[users_raw[uid]['Job']]
    ))
    print('为该用户推荐的电影：')
    for i, mid in enumerate(top_k_movies):
        # 推荐电影
        movie_recommend = movies_raw[mid]
        print('{0}: MovieID: {1}    Movie Title: {2}    Movie Type: {3}'.format(
            i + 1, movie_recommend['Movie_ID'], movie_recommend['Movie_Title'], movie_recommend['Movie_Type']
        ))
        # 获取相似用户 (对推荐电影评分最高者)
        movie_ratings = rank_matrix[mid].sort_values(ascending=False)
        for j in range(len(movie_ratings)):
            similar_user = movie_ratings.index[j]
            if similar_user in similar_users:
                continue
            else:
                similar_users.append(similar_user)
                break
    print('为该用户推荐的相似用户：')
    for i, similar_user_id in enumerate(similar_users):
        user_recommend = users_raw[similar_user_id]
        print('{0}: UserID: {1}    Gender:{2}    Age:{3}    Occupation:{4}'.format(
            i + 1, similar_user_id, user_gender[user_recommend['Gender']],
            user_age[user_recommend['Age']], user_job[user_recommend['Job']]
        ))


def recommend_Users(args, mid, rank_matrix):
    """
    根据 MovieID 得到喜欢看该电影的用户与他们喜欢看的电影 (相似电影)
    """
    users_raw, movies_raw, _, _ = load_preprocessed_data(args)
    movie_ratings = rank_matrix[mid].sort_values(ascending=False)
    top_k_users = list(movie_ratings.index)[0:args.recommend_num]
    similar_movies = []
    print('该电影信息：' + 'MovieID: {0}    Movie Title: {1}    Movie Type: {2}'.format(
        mid, movies_raw[mid]['Movie_Title'], movies_raw[mid]['Movie_Type']
    ))
    print('喜欢看这部电影的用户：')
    for i, uid in enumerate(top_k_users):
        user_recommend = users_raw[uid]
        print('{0}: UserID: {1}    Gender:{2}    Age:{3}    Occupation:{4}'.format(
            i + 1, uid, user_gender[user_recommend['Gender']],
            user_age[user_recommend['Age']], user_job[user_recommend['Job']]
        ))
        user_ratings = rank_matrix.loc[uid].sort_values(ascending=False)
        for j in range(len(user_ratings)):
            similar_movie = movie_ratings.index[j]
            if similar_movie in similar_movies:
                continue
            else:
                similar_movies.append(similar_movie)
                break
    print('他们也在看：')
    for i, similar_movie_id in enumerate(similar_movies):
        movie_recommend = movies_raw[similar_movie_id]
        print('{0}: MovieID: {1}    Movie Title: {2}    Movie Type: {3}'.format(
            i + 1, movie_recommend['Movie_ID'], movie_recommend['Movie_Title'], movie_recommend['Movie_Type']
        ))
\end{lstlisting}
\end{spacing}

\section{实验结果}
\subsection{推荐结果}
\xiaosi\begin{spacing}{1.5}
（1）矩阵分解结合深度学习方法\par
为编号为2021的用户推荐电影与相似用户：\par
\clearpage
\begin{figure}[h]
\setlength{\abovecaptionskip}{0.0cm}
\setlength{\belowcaptionskip}{-0.cm}
\centering
\includegraphics[width=1\textwidth]{Figure/dl1.png}
\end{figure}\par
\par
为编号为2021的电影相似电影与相关用户：
\begin{figure}[h]
\setlength{\abovecaptionskip}{0.0cm}
\setlength{\belowcaptionskip}{-0.cm}
\centering
\includegraphics[width=1\textwidth]{Figure/dl2.png}
\end{figure}\par
\par
（2）SimRank 算法\par
为编号为2021的用户推荐电影与相似用户：\par
\begin{figure}[h]
\setlength{\abovecaptionskip}{0.0cm}
\setlength{\belowcaptionskip}{-0.cm}
\centering
\includegraphics[width=1\textwidth]{Figure/svd1.png}
\end{figure}\par
\par
为编号为2021的电影推荐相似电影：\par
\begin{figure}[H]
\setlength{\abovecaptionskip}{0.0cm}
\setlength{\belowcaptionskip}{-0.cm}
\centering
\includegraphics[width=1\textwidth]{Figure/svd2.png}
\end{figure}\par
\par
\clearpage


\subsection{模型对比}
在预先划分的10000条数据构成的测试集上比较SimRank算法与矩阵分解算法的MSE误差，结果如下：\par
\begin{table}[h]
\centering
\caption{各模型基本信息与MSE指标}
\label{tab:1}
\begin{tabular}{cccc}
\toprule[2pt]
Models & Parameters & Time Cost & MSE  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
SimRank & K=10, C=1 & 14m37.2s & 2.083 \\
SimRank & K=10, C=0.8& 15m32.6s & \textbf{1.657} \\
\hline\noalign{\smallskip}
FunkSVD+DL & 4epochs, lr=5e-4, bs=512 & 49m26.7s & 0.899 \\
FunkSVD+DL & 4epochs, lr=3e-4, bs=512 & 51m18.2s & 0.880 \\
FunkSVD+DL & 5epochs, lr=3e-4, bs=512 & 61m63.1s & \textbf{0.865} \\
\toprule[2pt]
\end{tabular}
\end{table}
可见图论方法计算时间较短，但误差较高；矩阵分解结合深度学习方法训练时间长但同时效果更好。
\end{spacing}

\section{结语与心得}
\xiaosi\begin{spacing}{1.5}


\end{spacing}
\clearpage


\printbibliography%[heading=bibliography,title=参考文献]
\end{document}